{"config":{"lang":["zh","en","ja"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"Home","text":"<p>```markdown</p>"},{"location":"#androidworld-a-dynamic-benchmarking-environment-for-autonomous-agents","title":"AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents","text":""},{"location":"#authors","title":"Authors","text":"<ul> <li>Christopher Rawles* (Google DeepMind)</li> <li>Sarah Clinckemaillie\u2020 (Google)</li> <li>Yifan Chang\u2020 (Google)</li> <li>Jonathan Waltz (Google)</li> <li>Gabrielle Lau (Google)</li> <li>Marybeth Fair (Google)</li> <li>Alice Li (Google DeepMind)</li> <li>William Bishop (Google DeepMind)</li> <li>Wei Li (Google DeepMind)</li> <li>Folawiyo Campbell-Ajala (Google DeepMind)</li> <li>Daniel Toyama (Google DeepMind)</li> <li>Robert Berry (Google DeepMind)</li> <li>Divya Tyamagundlu (Google)</li> <li>Timothy Lillicrap (Google DeepMind)</li> <li>Oriana Riva (Google DeepMind)</li> </ul> <p>* Lead contributor. Contact: crawles@google.com \u2020 Equal contribution.</p>"},{"location":"#abstract","title":"Abstract","text":"<p>Autonomous agents that execute human tasks by controlling computers can enhance human productivity and application accessibility. However, progress in this field will be driven by realistic and reproducible benchmarks. We present AndroidWorld, a fully functional Android environment that provides reward signals for 116 programmatic tasks across 20 real-world Android apps. Unlike existing interactive environments, which provide a static test set, AndroidWorld dynamically constructs tasks that are parameterized and expressed in natural language in unlimited ways, thus enabling testing on a much larger and more realistic suite of tasks. To ensure reproducibility, each task includes dedicated initialization, success-checking, and tear-down logic, which modifies and inspects the device's system state.</p> <p>We experiment with baseline agents to test AndroidWorld and provide initial results on the benchmark. Our best agent can complete 30.6% of AndroidWorld's tasks, leaving ample room for future work. Furthermore, we adapt a popular desktop web agent to work on Android, which we find to be less effective on mobile, suggesting future research is needed to achieve universal, cross-platform agents. Finally, we also conduct a robustness analysis, showing that task variations can significantly affect agent performance, demonstrating that without such testing, agent performance metrics may not fully reflect practical challenges. AndroidWorld and the experiments in this paper are available at https://github.com/google-research/android_world.</p>"},{"location":"#introduction","title":"Introduction","text":"<p>Autonomous agents that interpret natural language instructions and operate computing devices can provide enormous value to users by automating repetitive tasks, augmenting human intelligence, and accomplishing complex workflows. However, a key research challenge remains the realistic evaluation of these agents in real-world settings. Despite growing enthusiasm for building autonomous agents, most existing approaches for evaluation compare an agent's actions at each step to a previously collected human demonstration. Measuring performance in this way can be misleading because when performing tasks online in real environments agents can take multiple paths to solve tasks, environments may behave non-deterministically, and agents can dynamically learn from mistakes to correct their actions. For this reason, online evaluation of agents in realistic environments able to reward task outcome provides a gold standard for evaluation. While there is an emerging body of work to address this need across different environments, there is no comprehensive solution for mobile platforms, such as Android, which are used by billions of users and therefore represent environments in which automation agents may be very productively employed. We introduce AndroidWorld to address this.</p> <p>At its core, AndroidWorld offers a reliable means of obtaining reward signals for tasks performed by agents in realistic mobile environments. Reward signals are quantitative metrics that indicate functional correctness of a task, i.e., is the stated goal achieved? For example, for the task \"Send a text message to Jane confirming I'll be there,\" a positive reward indicates that the relevant message has been sent. Unlike simulated environments or games, real-world apps and websites do not inherently offer explicit reward signals. While human or LLM-based judges can be employed to reward the outcome of a task, these approaches scale poorly or are not fully reliable, respectively. Alternatively, environments for autonomous agents which provide automated ground-truth rewards for complex workflows have been developed. We find two problems with these environments. First, they are constrained to desktop computing environments, overlooking the mobile domain, which is of paramount importance given the ubiquity and diversity of mobile devices in the real world. Secondly, they are limited in their real-world diversity and scale. Crucially, unlike in real-world scenarios where conditions and task inputs vary widely, these environments support only static test specifications, meaning that when task parameters deviate, the reward signal is likely to break.</p> <p></p> <p>We seek to develop a comprehensive benchmark that addresses the limitations of the existing approaches above for evaluating automation agents in mobile environments. AndroidWorld does this by spanning 20 Android apps on a total of 116 programmatic tasks to provide ground truth-rewards. Unlike existing test environments (with MiniWoB++ being a notable exception), each task in AndroidWorld is dynamically instantiated using randomly-generated parameters, challenging agents with millions of unique task goals and conditions. While MiniWob++ consists of simple, synthetic websites, AndroidWorld leverages actual Android applications. A main challenge that AndroidWorld must address is how to ensure that reward signals are durable when using real-world applications and varying task parameters dynamically. AndroidWorld's key insight is to leverage the extensive and consistent state management capabilities of the Android operating system, using the same mechanisms that the apps themselves utilize to store and update data.</p> <p>In addition to providing a comprehensive benchmark, AndroidWorld is lightweight, requiring only 2 GB of memory and 8 GB of disk space, and is designed with convenience in mind. It connects agents to the Android OS by leveraging the Python library AndroidEnv to connect to the freely available Android Emulator. In addition to the 116 Android tasks, we extend AndroidWorld with web tasks by integrating the MiniWoB++ benchmark into it.</p> <p>To demonstrate AndroidWorld's usefulness as a benchmark, we build and release a multi-modal agent, M3A (Multimodal Autonomous Agent for Android), and establish state-of-the-art results on AndroidWorld. We analyze M3A's performance using both multimodal and text-only input, and we observe that while multimodal perception can improve performance in some cases, it generally does not outperform the text-only approach. On AndroidWorld, M3A achieves a 30.6% success rate, which surpasses that of a web agent adapted for Android but remains significantly lower than the human success rate of 80.0%. In pursuit of building robust UI control agents, our study includes comprehensive tests under varied real-world conditions, demonstrating significant performance variations primarily driven by changes in intent parameters.</p> <p>Overall, we make the following contributions: (i) the creation of a new, highly diverse and realistic mobile UI control agent environment; (ii) establishment of benchmark performance with a state-of-the-art multimodal agent, and (iii) a careful analysis demonstrating the need to evaluate agents across variable task parameters and conditions due to the inherent stochasticity in both models and environments.</p>"},{"location":"#methods","title":"Methods","text":""},{"location":"#android-for-autonomous-agents","title":"Android for Autonomous Agents","text":"<p>Android is an ideal environment for developing autonomous agents. It is the most widely-used OS globally and is highly flexible for research, while providing an open world of the Web and over 2M apps for agents to operate in. Using emulation, an Android environment is easy to deploy, does not require specialized hardware, and can be run on a laptop. Android Virtual Devices or emulator images are well suited for research as they are self-contained, easy to distribute, and configurable.</p> <p>Compared to desktop environments, mobile environments, like Android, pose unique research challenges for computer control agents. On one hand, mobile UIs tend to be simpler than their desktop counterparts because of their smaller screen size. On the other hand, the action space on mobile devices is more complicated and more actions can be required to complete tasks. Precise gestures are needed to fully operate the UI, such as when navigating a carousel widget, long-pressing on a widget, or performing multi-finger gestures to zoom in. Since it is an OS, Android is a fully open environment compared to web-browser-only environments. Android's flexibility is also reflected in its action space; in addition to UI actions (click, scroll, type, etc.), Android provides function-calling APIs, such as sending a text message, for example, which allow computer control agents to utilize a broader action space.</p>"},{"location":"#the-observation-and-action-space","title":"The Observation and Action Space","text":"<p>AndroidWorld provides an interface for agents to receive observations and execute actions on Android. It uses AndroidEnv and the Android Device Bridge to facilitate interaction between Android and the agent. The observation space consists of a full-resolution screenshot and a UI tree representation developed for accessibility purposes. The action space is similar to that which humans use, consisting of gestures (i.e., tapping, long-press, and swiping), typing, and navigation buttons (i.e., go home and go back). In addition to these naturalistic actions, AndroidWorld exposes a limited set of function calling APIs, such as <code>send_text_message</code>, to help agents accomplish goals.</p>"},{"location":"#reproducible-and-parameterized-tasks","title":"Reproducible and Parameterized Tasks","text":"<p>AndroidWorld consists of a suite of 116 tasks, spread across 20 diverse applications. These tasks simulate practical, everyday activities, including note-taking, scheduling appointments, communicating through messaging, and interacting with system utilities. The suite consists of open-source apps and built-in Android system apps, such as Settings and Contacts. As rated by humans, the tasks vary in difficulty, duration, and categories.</p> <p>To achieve a high degree of reproducibility in real-world scenarios, AndroidWorld precisely controls the OS and app states in several ways. The Android OS is fixed, consisting of a Pixel 6 emulator running Android 13 with a fixed time on the date October 15th, 2023. All applications in AndroidWorld are fully-functional and consist of both open-source apps and OS-level apps included with Android. For the open-source apps, AndroidWorld maintains a constant environment by installing a fixed version of each app, acquired from F-Droid. OS-level apps' versions are determined by the Android OS, which is also fixed. To maintain a reproducible environment, AndroidWorld utilizes apps that do not require login/authentication and can store their application data on device.</p> <p>In addition to managing the states of apps and operating systems, AndroidWorld precisely defines and controls the state during task execution. Each task has its own unique setup, reward determination logic, and teardown procedures, ensuring a fully reproducible suite of tasks.</p> <p>Automatic task parameterization is a critical mechanism, unique to AndroidWorld, to evaluate agents on a much larger and more realistic suite of tasks than current benchmarks support. Achieving this requires significantly more effort than randomly generating new task parameters because it involves developing evaluation logic that remains valid across different task instantiations. It is exactly through its careful state management that in addition to reproducibility AndroidWorld ensures that the reward mechanisms function correctly. Task parameters, initialized randomly at the start of each task based on a controlled random seed, dictate the initial state and influence reward outcomes. Similarly to MiniWoB++, AndroidWorld consists of a practically infinite set of varying initial conditions and success criteria.</p> <p>This approach provides more granular analyses of agents' adaptability \u2014 a vital attribute for real-world deployment. Beyond testing agent robustness, the dynamic construction of tasks supports the use of online learning methodologies, particularly reinforcement learning. It also simplifies the generation of distinct train/test datasets, facilitating supervised learning experiments.</p>"},{"location":"#durable-rewards-from-system-state","title":"Durable Rewards from System State","text":"<p>AndroidWorld provides reward signals by managing application state using the Android Debug Bridge (adb). With the adb tool AndroidWorld has complete access to system resources including the file system, application databases, and system settings. Determining reward signals from system state has several benefits. It is highly accurate because an application's state can be quickly inspected and manipulated using the same mechanisms that the app itself utilizes. Using the underlying system state is much more durable than matching superficial UI changes. Additionally, it facilitates easy re-use across disparate apps, which tend to use the same underlying caching mechanisms. For instance, logic for checking existence of a specific file is used across many unrelated applications, including those for file management, note-taking, and media playback.</p>"},{"location":"#task-composability","title":"Task Composability","text":"<p>In addition to facilitating accurate and reusable evaluations, inferring a task's success from system state makes it easy to create composite tasks by combining together existing tasks. For example, the task \"Create a calendar event with {details} and text the details to {contact}\" was created by combining together two existing tasks for creating a calendar event and for sending a text message, which is possible because each task initialization and success detection logic is hermetic. Composite tasks tend to be more challenging because of their complexity, although they provide partial rewards based on completion of sub tasks, to help facilitate hill climbing.</p>"},{"location":"#integrating-miniwob","title":"Integrating MiniWoB++","text":"<p>We implement MiniWoB++ in the AndroidWorld framework and term it MobileMiniWoB++. Each MobileMiniWoB++ task is instantiated using the standard AndroidWorld interface, inheriting from TaskEval base class, and contains methods like initialize_state and is_successful. Since MiniWoB++ leverages JavaScript for task configuration and success detection, we built a WebView app to communicate between Python and the app. For instance, the is_successful method of each task retrieves the reward value from the WebView app via an Android intent.</p> <p>MobileMiniWoB++ introduces modifications in both observations and actions compared to the original benchmark. For example, HTML5 <code>&lt;input&gt;</code> elements are rendered with native Android UI widgets like the date-picker, enhancing the realism of the tasks. MobileMiniWoB++ uses the same observation space as the Android tasks (accessibility tree and screenshot). Notably, it does not include the DOM as in the original implementation. The action space from AndroidWorld is retained. We manually review and test each task to ensure they are solvable. We excluded twelve of the original tasks that failed to render correctly on Android, presented compatibility issues with the touch interface, or required near real-time interaction, which poses challenges on emulators. Overall, AndroidWorld supports 92 MiniWoB++ tasks.</p>"},{"location":"#experimental-results","title":"Experimental Results","text":"<p>To test AndroidWorld's applicability for autonomous agents, we develop and test a state-of-the-art agent and its variants across all 20 apps and 116 tasks, as well as on MobileMiniWoB++.</p>"},{"location":"#computer-control-agents","title":"Computer Control Agents","text":""},{"location":"#m3a","title":"M3A","text":"<p>We develop a multimodal autonomous agent for Android, M3A. It is zero-shot, integrating ReAct-style and Reflexion-style prompting to consume user instructions and screen content, reason, take actions, and update its decision-making based on the outcome of its actions.</p> <p>In the first stage, M3A generates an action, represented in JSON, and reasoning for that action. To generate this output, the agent is provided with a list of available action types, guidelines for operating the phone, and a list of UI elements derived from the Android accessibility tree's leaf nodes. The agent receives the current screenshot and a Set-of-Mark (SoM) annotated screenshot, which includes bounding boxes with numeric labels on the top-left corner for each UI element. The agent attempts to execute outputted action by referencing the specific mark (if applicable). In addition to the multimodal agent, we have developed a text-only variant that consumes the screen represented using the accessibility tree and selects the relevant action in JSON format.</p> <p>After executing an action, M3A reflects on its effect by observing any state changes that may have occurred. During this stage, the agent is provided with available action types, general operating guidelines, the actual action taken, and its reasoning, as well as before-and-after UI states, represented by UI element representations and screenshots with SoM annotations. We request the agent to provide a concise summary of this step, including the intended action, success or failure, potential reasons for failure, and recommendations for subsequent actions. This summary will serve as the action history and be used for future action selection.</p>"},{"location":"#seeact-baseline","title":"SeeAct Baseline","text":"<p>We implement a baseline agent based on SeeAct, which was originally designed for GPT-4V for web navigation. Specifically, we implement the best-performing variant, SeeAct_choice, which grounds actions via textual choices. We implement SeeAct for the Android environment to evaluate how an existing model that performs well on web tasks can be adapted and applied to Android.</p> <p>To accommodate the Android environment, we adapt SeeAct in several ways. Firstly, we augment the action space from the original SeeAct implementation to support actions needed for mobile, including scroll, long press, navigate home and back, and open app actions. Secondly, in lieu of the DOM, which is not available for Android apps, we utilize the accessibility tree to construct candidate UI actions. Due to the lack of the DOM representation, we do not use the bespoke ranker model from the original implementation. However, we observe that after applying a filtering heuristic to remove non-interactable elements, the majority of screens contains less than 50 candidate elements.</p>"},{"location":"#experimental-results_1","title":"Experimental Results","text":"<p>We evaluate M3A and SeeAct on AndroidWorld and MobileMiniWoB++. We set the seed to 30 and provide a task-specific step budget. We use Gemini 1.5 Pro and GPT-4 Turbo as base models. For MobileMiniWoB++, we evaluate on a subset of 62 tasks, consistent with recent studies.</p> Agent Input Base model SR_AndroidWorld SR_MobileMiniWoB Human screen N/A 80.0 100.0 M3A a11y tree GPT-4 Turbo 30.6 59.7 M3A a11y tree Gemini 1.5 Pro 19.4 57.4 M3A SoM (screen + a11y tree) GPT-4 Turbo 25.4 67.7 M3A SoM (screen + a11y tree) Gemini 1.5 Pro 22.8 40.3 SeeAct SoM (screen + a11y tree) GPT-4 Turbo 15.5 66.1 <p>Although the agents have far from human performance, they demonstrate out-of-the-box capabilities in operating mobile UIs, exhibiting basic understanding and control capabilities of UIs. They can perform a variety of actions, including long-press, scrolling to search for information, and revising their plan if actions do not work out. The best performance is obtained for M3A when using GPT-4. On AndroidWorld the SoM-based variant is less performant, while on MobileMiniWoB it performs best. A similar result was obtained in recent work in the context of computer agents for desktop applications. We posit SoM plays a more critical role in MobileMiniWoB tasks due to the often incomplete accessibility tree, compared to that of native Android apps.</p>"},{"location":"#analysis","title":"Analysis","text":"<p>Agents have difficulty understanding mobile UIs, often failing to detect visual cues that are essential for task completion. Additionally, agents struggle with certain UI patterns and affordances, and when they make reasoning mistakes, they often lack the capability to explore and adapt as humans do. Moreover, agents sometimes struggle with tasks that simply involve confirming system states, e.g., confirming the WiFi is turned on, suggesting challenges in both task and screen understanding.</p> <p>The agents struggle with grounding, particularly when executing precise interactions, such as manipulating text or operating sliders, and they are often unable to recover from mistyping errors. In addition, for tasks that demand memory, such as performing transcriptions across apps, multiplying numbers, or scrolling, the agents struggle as they are unable to \"remember\" content.</p> <p>SeeAct performs less effectively than M3A on the AndroidWorld task suite and similarly on MobileMiniWoB, reflecting its optimization for web rather than mobile environments. It struggles with mobile-specific actions like long-presses and swipes, and often fails to select appropriate actions due to not incorporating screen elements during action generation. Memory-intensive tasks are particularly challenging, as SeeAct</p>"}]}